version: "3.9"

services:
  influxdb:
    image: influxdb:2.7
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: logflow
      DOCKER_INFLUXDB_INIT_PASSWORD: logflow1234
      DOCKER_INFLUXDB_INIT_ORG: logflow
      DOCKER_INFLUXDB_INIT_BUCKET: logflow
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: logflow-dev-token
      DOCKER_INFLUXDB_INIT_RETENTION: 30d
    volumes:
      - influx-data:/var/lib/influxdb2
    healthcheck:
      test: ["CMD", "influx", "ping", "--host", "http://localhost:8086"]
      interval: 10s
      timeout: 5s
      retries: 12
    mem_limit: "512m"
    cpus: "0.8"

  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile.app
    environment:
      INFLUX_URL: http://influxdb:8086
      INFLUX_TOKEN: logflow-dev-token
      INFLUX_ORG: logflow
      INFLUX_BUCKET: logflow
      ALLOW_ORIGINS: "*"
    ports:
      - "8080:9000"            # 그대로 유지 (대시보드/ba.js)
    depends_on:
      influxdb:
        condition: service_healthy
    mem_limit: "256m"
    cpus: "0.5"
  
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    # 모델/포트/스레드 등을 'command'로 명시 (환경변수 대신 확실하게)
    command: [
      "-m", "/models/Qwen2.5-0.5B-Instruct-Q3_K_M.gguf",
      "-c", "512", "-t", "2",
      "--ubatch", "16", "-b", "16",
      "-ngl", "0",
      "--host", "0.0.0.0", "--port", "8000"
    ]
    volumes:
      - ../models:/models:ro   # 스크립트가 ../models에 받도록 해뒀으니 여기를 마운트
    ports:
      - "8081:8000"            # 호스트 8081로 노출 (app과 충돌 피함)
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 12
    mem_limit: "512m"
    cpus: "0.8"
    restart: unless-stopped

volumes:
  influx-data:
